# Case-study: VIKBA05 Computer-aided Data Transformation

> This case study is about a course that is currently being taught. As such, it is continuously updated and may contain incomplete, unclear or misleading information. Feel free to contact me at [my e-mail address](mailto:honza.martinek@gmail.com) or discuss it in the github comments.

Peer Blender is currently being employed to teach a freshman course *VIKBA05 Computer-aided Data Transformation* at Masaryk University, Brno (Czech Republic).

There's 101 students in the course and the number of assessments in each unit is five (hence there should be 505 assessments every week). The course is worth 4 ECTS, [that means](http://ec.europa.eu/education/ects/users-guide/assets/ects_users-_guide_web.pdf) that average student should spend 100—120 hours completing it. In a semester that is 12 weeks long, that translates into 8—10 hours every week.

To complete the course, the students need to 

- fulfill the weekly assignments (the average rating from every unit is added up into a final grade)
- write 5 assessments of others work every week (otherwise their score from their own assignment is lost)
- complete the final test with at least 65 % points (for a 90+ % success, there's a 3-point bonus)

## Structure of a learning unit

The course has a week-long teaching cycle, with a seminar every friday. All deadlines are set on 8pm. The [four phases](../about.md#phases) are set like this:

- **learning**: friday to monday (afternoon seminar)
- **assessment**: monday evening to thursday
- **objections**: thursday evening to friday (afternoon seminar)
- **final phase**: friday evening
    
In the 90 minutes-long seminar, there's

- **opening** (15 mins), summarizing the main points what's been the topic of the week
- **discussion** (60 mins) about problems with the content or the course itself
- **introduction** (15 mins) of next week's topic

The supposed *ideal* distribution of a student's work in a week is:

- **learning** (incl. assignment fulfilling): 3.5 hours
- **assessment**: 4 hours
- **objections**: 0.5 hours (preferrably)
- **final phase**: 1.5 hours

## Structure of the course

- **Working with text**
    - September 25: Intro meeting
    - October 2: Typography and semantics
    - October 9: Markup languages: HTML
    - October 16: Markup languages: Markdown
    - October 23: Advanced text editors, find & replace, regular expressions
- **Working with tabular data**
    - October 31: Describing things with parameters
    - November 7: Spreadsheets 1: simple formulas
    - November 14: Spreadsheets 2: joining data
- **Working with a database**
    - November 14: Intro to SQL — SELECT
    - November 28: Joining the tables
- **Intro into scripting**
    - December 4: Variables and functions
    - December 11: Conditions and cycles
    - December 18: Writing my own script

## Notes

(These are really just self-reported notes. When there's any „because of“ deduction, it's just a thought, not a proven hypothese.)

### The good

- For better or worse, the system worked (both the procedure and the software).
- Some students openly say it worked for them.
- The distribution of A-F marks is quite similar to the last-year's course.
- Students got used to using the discussion forum to help each other.
- There were at least two study-groups working inside the course.
- The seminars were mandatory – I think this worked well in the first semestr. In the subsequent semestr, this might not be needed.

### The bad (unsorted)

- Some problems from the student feedback (It's hard to discern misapprehension and justified criticism in student's complaints about the contents of seminars and the learning process. For my purposes, I prefer to render them as justified criticism unless proven else.):
    - The seminars were described as chaotic.
    - The assignments were described as too difficult.
    - The contents of the course were described as ‘not useful’ (I take that as a sign that the tasks were not presented in a meaningful context for the student).
- To contents of the course were in the making well after the course's start. This caused some unpredictability and several times I have failed to provide good learning materials. This was in practice indistinguishable from the learning goal of ‘being able to familiarize with the issue contents by myself’ and mostly obscured it as the learning curve was too steep.
- The start of the semester was harsh, as I had to focus on debugging instead of keeping an eye on the reviews quality - hence some of them are quite bad (reviewers giving stars to incomplete tasks, copy-pasting the same reviews over and over etc.). Some form of a review calibration is probably needed the next time.
- There were technical problems in the start of the semester (fatal errors when reaching a new phase in the process, failing to display a part of the solution to reviewers once etc.) and my own errors in the end of the semester (failing to provide assignment instructions or review rubrics in time, resulting into several students running into badly structured page – only some of them noticing and therefore having to repeat the task when the instructions are fixed).
- The concept of „helpers“ did not work out. There were some important benefits from having them, but as a whole, the concept needs to be reformed.
    - The helpers from inside of the course were competent only in some of the units. In those, they were able to deliver the desired results (lists of common problems, reporting & fixing questionable reviews).
    - I was not able to make use of their work in the second half of the course, as there was a little time to ask for the results (that could be prevented by more careful planning).
    - The research seminar consisted of several people focused mainly on the teaching/learning, not so much on the contents of the course. One of the members, also part-teacher, provided a lot of help in the discussion forum (that has basically taken off because of his initiative) and hosted a seminar.
    - There were no clear instructions and evaluation metrics for the helpers – it left both the students in uncertainty about their progress and marks. (I thought we would be able to work more as a team - that failed for the lack of time on my side.)
- Because of the initial software problems, some students perceived the seminars as software-focused. 
- The seminars were used to demonstrate common problems – for some students this was frustrating. I should've focused more on the student-on-student interaction, somehow grouping students. (There was a problem with the rooms: I supposed that there will be some kind of a room with movable chairs to enable better interaction between students, but in the first one chairs could not be moved, so we moved for the last few sessions into a room that at least had electrical outlets.)
- My goal was to have a research team that follows the work I and the students do in the course – this failed completely as I was not able to manage the team. This research is not finished as I write this summary (mid-feb 2016).
- One of the foundations were to provide tasks well formed around the (revised) Bloom's taxonomy. The categorization and formulation of the assignments in the second half of the course was not ideal.
- Because of the imperfectly written review rubrics, the value of this learning phase was lessened & its purpose made unclear.
- For some of the students (mostly distance-learners), the intervals were too short, making them unable to keep up with the others.

### Next steps?

- Strict review calibration in the start and in the course of the semestr.
- Better formulated rubrics are needed.
- In the follow-up course, the units will run for two weeks, one theoretical and one practical.
- Unit-related ‘helpers’.
